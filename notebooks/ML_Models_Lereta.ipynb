{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lereta Intelligence Agent - ML Models\n",
        "\n",
        "**Training 3 Machine Learning Models for Tax & Flood Intelligence**\n",
        "\n",
        "This notebook trains 3 ML models for the Lereta Intelligence Agent:\n",
        "1. **TAX_DELINQUENCY_PREDICTOR** - Predicts property tax delinquency risk\n",
        "2. **CLIENT_CHURN_PREDICTOR** - Predicts client churn risk\n",
        "3. **LOAN_RISK_CLASSIFIER** - Classifies loans by risk level (LOW/MEDIUM/HIGH)\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "**⚠️ IMPORTANT: Execute files 01-04 BEFORE running this notebook!**\n",
        "\n",
        "This notebook requires feature views created in file 04:\n",
        "- **V_TAX_DELINQUENCY_FEATURES** (created in sql/views/04_create_views.sql)\n",
        "- **V_CLIENT_CHURN_FEATURES** (created in sql/views/04_create_views.sql)\n",
        "- **V_LOAN_RISK_FEATURES** (created in sql/views/04_create_views.sql)\n",
        "\n",
        "**Correct Execution Order**:\n",
        "1. Execute sql/setup/01_database_and_schema.sql\n",
        "2. Execute sql/setup/02_create_tables.sql\n",
        "3. Execute sql/data/03_generate_synthetic_data.sql (10-20 min)\n",
        "4. Execute sql/views/04_create_views.sql ← **Creates ML feature views**\n",
        "5. Run this notebook (trains models)\n",
        "6. Execute sql/ml/07_ml_model_wrappers.sql (wraps trained models)\n",
        "7. Execute sql/agent/08_create_ai_agent.sql\n",
        "\n",
        "**Packages**: `snowflake-ml-python=1.19.0`, `scikit-learn=1.6.1`, `pandas=2.2.3`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.ml.modeling.ensemble import RandomForestClassifier\n",
        "from snowflake.ml.modeling.xgboost import XGBClassifier\n",
        "from snowflake.ml.modeling.preprocessing import OrdinalEncoder, StandardScaler\n",
        "from snowflake.ml.modeling.pipeline import Pipeline\n",
        "from snowflake.ml.registry import Registry\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✅ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get current session\n",
        "session = Session.builder.getOrCreate()\n",
        "\n",
        "# Set context\n",
        "session.use_database(\"LERETA_INTELLIGENCE\")\n",
        "session.use_schema(\"ML_MODELS\")\n",
        "session.use_warehouse(\"LERETA_WH\")\n",
        "\n",
        "print(\"✅ Session configured\")\n",
        "print(f\"Database: {session.get_current_database()}\")\n",
        "print(f\"Schema: {session.get_current_schema()}\")\n",
        "print(f\"Warehouse: {session.get_current_warehouse()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Model Registry\n",
        "registry = Registry(\n",
        "    session=session,\n",
        "    database_name=\"LERETA_INTELLIGENCE\",\n",
        "    schema_name=\"ML_MODELS\"\n",
        ")\n",
        "\n",
        "print(\"✅ Model Registry initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 1: Tax Delinquency Prediction\n",
        "\n",
        "### Business Problem\n",
        "Predict which properties are likely to become delinquent on property taxes in the next 90 days. This enables proactive outreach to clients and borrowers to prevent delinquencies.\n",
        "\n",
        "### Features\n",
        "- Property characteristics (type, assessed value)\n",
        "- Tax amount and jurisdiction\n",
        "- Historical payment patterns\n",
        "- Days since last payment\n",
        "- Client service quality score\n",
        "- Loan characteristics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify feature view exists, then load data\n",
        "try:\n",
        "    tax_delinquency_df = session.table(\"LERETA_INTELLIGENCE.ANALYTICS.V_TAX_DELINQUENCY_FEATURES\")\n",
        "    record_count = tax_delinquency_df.count()\n",
        "    print(f\"✅ Loaded {record_count} records for tax delinquency prediction\")\n",
        "    tax_delinquency_df.show(5)\n",
        "except Exception as e:\n",
        "    print(\"❌ ERROR: V_TAX_DELINQUENCY_FEATURES view not found!\")\n",
        "    print(\"Please execute sql/ml/07_ml_model_wrappers.sql before running this notebook.\")\n",
        "    print(f\"Error details: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data for training and testing\n",
        "train_df, test_df = tax_delinquency_df.random_split([0.8, 0.2], seed=42)\n",
        "\n",
        "# Drop ID column not needed for training\n",
        "train_df = train_df.drop(\"TAX_RECORD_ID\")\n",
        "test_df = test_df.drop(\"TAX_RECORD_ID\")\n",
        "\n",
        "print(f\"Training set: {train_df.count()} records\")\n",
        "print(f\"Test set: {test_df.count()} records\")\n",
        "\n",
        "# Create tax delinquency prediction pipeline - optimized for speed\n",
        "tax_delinquency_pipeline = Pipeline([\n",
        "    (\"Encoder\", OneHotEncoder(\n",
        "        input_cols=[\"PROPERTY_TYPE\", \"FLOOD_ZONE\", \"JURISDICTION_TYPE\", \"LOAN_TYPE\", \"CLIENT_TYPE\", \"LOAN_STATUS\", \"CLIENT_STATUS\"],\n",
        "        output_cols=[\"PROPERTY_TYPE_ENC\", \"FLOOD_ZONE_ENC\", \"JURISDICTION_TYPE_ENC\", \"LOAN_TYPE_ENC\", \"CLIENT_TYPE_ENC\", \"LOAN_STATUS_ENC\", \"CLIENT_STATUS_ENC\"],\n",
        "        drop_input_cols=True,\n",
        "        handle_unknown=\"ignore\"\n",
        "    )),\n",
        "    (\"Classifier\", RandomForestClassifier(\n",
        "        label_cols=[\"ACTUAL_DELINQUENT\"],\n",
        "        output_cols=[\"PREDICTED_DELINQUENT\"],\n",
        "        n_estimators=10,\n",
        "        max_depth=5,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "print(\"✅ Tax delinquency pipeline created (optimized for speed)\")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training tax delinquency prediction model...\")\n",
        "tax_delinquency_pipeline.fit(train_df)\n",
        "print(\"✅ Tax delinquency model trained\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model on test set\n",
        "test_predictions = tax_delinquency_pipeline.predict(test_df)\n",
        "test_results = test_predictions.select(\"ACTUAL_DELINQUENT\", \"PREDICTED_DELINQUENT\").to_pandas()\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy = accuracy_score(test_results['ACTUAL_DELINQUENT'], test_results['PREDICTED_DELINQUENT'])\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(\n",
        "    test_results['ACTUAL_DELINQUENT'], \n",
        "    test_results['PREDICTED_DELINQUENT']\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete existing model if it exists to force fresh registration\n",
        "try:\n",
        "    registry.delete_model(\"TAX_DELINQUENCY_PREDICTOR\")\n",
        "    print(\"✅ Deleted existing TAX_DELINQUENCY_PREDICTOR\")\n",
        "except:\n",
        "    print(\"No existing model to delete\")\n",
        "\n",
        "# Register model in Model Registry\n",
        "# Drop label column from sample data - model signature should only include features\n",
        "sample_data = train_df.drop(\"ACTUAL_DELINQUENT\").limit(100)\n",
        "\n",
        "registry.log_model(\n",
        "    model=tax_delinquency_pipeline,\n",
        "    model_name=\"TAX_DELINQUENCY_PREDICTOR\",\n",
        "    target_platforms=['WAREHOUSE'],\n",
        "    sample_input_data=sample_data,\n",
        "    comment=\"Predicts property tax delinquency risk\"\n",
        ")\n",
        "\n",
        "print(\"✅ TAX_DELINQUENCY_PREDICTOR registered in Model Registry\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 2: Client Churn Prediction\n",
        "\n",
        "### Business Problem\n",
        "Identify clients (financial institutions) at risk of canceling their Lereta subscriptions. This enables proactive retention efforts and improved customer success.\n",
        "\n",
        "### Features\n",
        "- Subscription characteristics (tier, billing cycle, property count)\n",
        "- Service utilization patterns\n",
        "- Support ticket volume and satisfaction\n",
        "- Revenue and transaction trends\n",
        "- Client profile and status\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load client churn feature data from pre-built view\n",
        "client_churn_df = session.table(\"LERETA_INTELLIGENCE.ANALYTICS.V_CLIENT_CHURN_FEATURES\")\n",
        "\n",
        "print(f\"✅ Loaded {client_churn_df.count()} records for client churn prediction\")\n",
        "client_churn_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering for churn prediction\n",
        "categorical_features_churn = ['CLIENT_TYPE', 'SERVICE_TYPE', 'SUBSCRIPTION_TIER', 'BILLING_CYCLE']\n",
        "numerical_features_churn = [\n",
        "    'SERVICE_QUALITY_SCORE', 'TOTAL_PROPERTIES', 'LIFETIME_VALUE', 'MONTHS_AS_CLIENT',\n",
        "    'MONTHLY_PRICE', 'PROPERTY_COUNT_LIMIT', 'USER_LICENSES', 'SUBSCRIPTION_DURATION_DAYS',\n",
        "    'TOTAL_SUPPORT_TICKETS', 'AVG_SATISFACTION_RATING', 'AVG_RESOLUTION_TIME', 'OPEN_TICKETS',\n",
        "    'TOTAL_TRANSACTIONS', 'TOTAL_REVENUE', 'AVG_TRANSACTION_AMOUNT'\n",
        "]\n",
        "\n",
        "# Split data\n",
        "train_churn_df, test_churn_df = client_churn_df.random_split([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"Training set: {train_churn_df.count()} rows\")\n",
        "print(f\"Test set: {test_churn_df.count()} rows\")\n",
        "\n",
        "# Build churn prediction pipeline with XGBoost\n",
        "client_churn_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        ('encoder', OrdinalEncoder(input_cols=categorical_features_churn, output_cols=categorical_features_churn)),\n",
        "        ('scaler', StandardScaler(input_cols=numerical_features_churn, output_cols=numerical_features_churn)),\n",
        "        ('classifier', XGBClassifier(\n",
        "            input_cols=categorical_features_churn + numerical_features_churn,\n",
        "            label_cols=['IS_CHURNED'],\n",
        "            n_estimators=150,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Client Churn Prediction Model...\")\n",
        "client_churn_model = client_churn_pipeline.fit(train_churn_df)\n",
        "print(\"✅ Model trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate churn model\n",
        "churn_predictions_df = client_churn_model.predict(test_churn_df)\n",
        "\n",
        "print(\"\\n=== Client Churn Model Performance ===\")\n",
        "accuracy_churn = accuracy_score(df=churn_predictions_df, y_true_col_name='IS_CHURNED', y_pred_col_name='OUTPUT_IS_CHURNED')\n",
        "precision_churn = precision_score(df=churn_predictions_df, y_true_col_name='IS_CHURNED', y_pred_col_name='OUTPUT_IS_CHURNED')\n",
        "recall_churn = recall_score(df=churn_predictions_df, y_true_col_name='IS_CHURNED', y_pred_col_name='OUTPUT_IS_CHURNED')\n",
        "f1_churn = f1_score(df=churn_predictions_df, y_true_col_name='IS_CHURNED', y_pred_col_name='OUTPUT_IS_CHURNED')\n",
        "\n",
        "print(f\"Accuracy:  {accuracy_churn:.4f}\")\n",
        "print(f\"Precision: {precision_churn:.4f}\")\n",
        "print(f\"Recall:    {recall_churn:.4f}\")\n",
        "print(f\"F1 Score:  {f1_churn:.4f}\")\n",
        "\n",
        "churn_predictions_df.select('CLIENT_ID', 'IS_CHURNED', 'OUTPUT_IS_CHURNED').show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register churn model\n",
        "session.use_schema(\"ANALYTICS\")\n",
        "\n",
        "model_name_churn = \"CLIENT_CHURN_PREDICTOR\"\n",
        "model_version_churn = \"v1\"\n",
        "\n",
        "print(f\"Registering model: {model_name_churn}_{model_version_churn}\")\n",
        "registry.log_model(\n",
        "    model=client_churn_model,\n",
        "    model_name=model_name_churn,\n",
        "    version_name=model_version_churn,\n",
        "    comment=\"Predicts client churn risk using XGBoost. Features: subscription metrics, support satisfaction, revenue trends, service utilization.\"\n",
        ")\n",
        "\n",
        "print(f\"✅ Model {model_name_churn} version {model_version_churn} registered successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 3: Loan Risk Classification\n",
        "\n",
        "### Business Problem\n",
        "Classify loans into risk categories (Low, Medium, High) based on tax compliance, flood zone risks, and property characteristics. This helps prioritize monitoring efforts and identify high-risk portfolios.\n",
        "\n",
        "### Features\n",
        "- Flood zone and insurance requirements\n",
        "- Tax payment history and delinquency status\n",
        "- Loan-to-value ratio\n",
        "- Property type and assessed value\n",
        "- Escrow account status\n",
        "- Jurisdiction tax rates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load loan risk feature data from pre-built view\n",
        "loan_risk_df = session.table(\"LERETA_INTELLIGENCE.ANALYTICS.V_LOAN_RISK_FEATURES\")\n",
        "\n",
        "print(f\"✅ Loaded {loan_risk_df.count()} records for loan risk classification\")\n",
        "loan_risk_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering for loan risk classification\n",
        "categorical_features_risk = [\n",
        "    'LOAN_TYPE', 'LOAN_STATUS', 'PROPERTY_TYPE', 'FLOOD_ZONE', \n",
        "    'PROPERTY_STATE', 'JURISDICTION_TYPE', 'CLIENT_TYPE'\n",
        "]\n",
        "numerical_features_risk = [\n",
        "    'LOAN_AMOUNT', 'LOAN_AGE_MONTHS', 'LOAN_TO_VALUE_RATIO', 'ASSESSED_VALUE',\n",
        "    'HIGH_FLOOD_RISK', 'TAX_AMOUNT', 'PENALTY_AMOUNT', 'TAX_RATE',\n",
        "    'TAX_PAID_ON_TIME', 'DAYS_PAYMENT_DELAY', 'SERVICE_QUALITY_SCORE'\n",
        "]\n",
        "\n",
        "# Split data\n",
        "train_risk_df, test_risk_df = loan_risk_df.random_split([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"Training set: {train_risk_df.count()} rows\")\n",
        "print(f\"Test set: {test_risk_df.count()} rows\")\n",
        "\n",
        "# Build loan risk classification pipeline\n",
        "loan_risk_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        ('encoder', OrdinalEncoder(input_cols=categorical_features_risk, output_cols=categorical_features_risk)),\n",
        "        ('scaler', StandardScaler(input_cols=numerical_features_risk, output_cols=numerical_features_risk)),\n",
        "        ('classifier', RandomForestClassifier(\n",
        "            input_cols=categorical_features_risk + numerical_features_risk,\n",
        "            label_cols=['RISK_LEVEL'],\n",
        "            n_estimators=120,\n",
        "            max_depth=12,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Loan Risk Classification Model...\")\n",
        "loan_risk_model = loan_risk_pipeline.fit(train_risk_df)\n",
        "print(\"✅ Model trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate loan risk model\n",
        "risk_predictions_df = loan_risk_model.predict(test_risk_df)\n",
        "\n",
        "print(\"\\n=== Loan Risk Classification Model Performance ===\")\n",
        "accuracy_risk = accuracy_score(df=risk_predictions_df, y_true_col_name='RISK_LEVEL', y_pred_col_name='OUTPUT_RISK_LEVEL')\n",
        "\n",
        "print(f\"Accuracy: {accuracy_risk:.4f}\")\n",
        "\n",
        "# Show sample predictions\n",
        "risk_predictions_df.select('LOAN_ID', 'RISK_LEVEL', 'OUTPUT_RISK_LEVEL', 'FLOOD_ZONE', 'DELINQUENT').show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register loan risk model\n",
        "session.use_schema(\"ANALYTICS\")\n",
        "\n",
        "model_name_risk = \"LOAN_RISK_CLASSIFIER\"\n",
        "model_version_risk = \"v1\"\n",
        "\n",
        "print(f\"Registering model: {model_name_risk}_{model_version_risk}\")\n",
        "registry.log_model(\n",
        "    model=loan_risk_model,\n",
        "    model_name=model_name_risk,\n",
        "    version_name=model_version_risk,\n",
        "    comment=\"Classifies loans into LOW/MEDIUM/HIGH risk categories using Random Forest. Features: flood zones, tax compliance, LTV ratio, property characteristics.\"\n",
        ")\n",
        "\n",
        "print(f\"✅ Model {model_name_risk} version {model_version_risk} registered successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "### Models Created\n",
        "\n",
        "1. **TAX_DELINQUENCY_PREDICTOR** (Random Forest)\n",
        "   - Predicts property tax delinquency risk\n",
        "   - Use case: Proactive client alerts, portfolio risk management\n",
        "\n",
        "2. **CLIENT_CHURN_PREDICTOR** (XGBoost)\n",
        "   - Predicts client subscription cancellation risk\n",
        "   - Use case: Customer retention, account management prioritization\n",
        "\n",
        "3. **LOAN_RISK_CLASSIFIER** (Random Forest)\n",
        "   - Classifies loans into risk levels (LOW/MEDIUM/HIGH)\n",
        "   - Use case: Portfolio risk assessment, monitoring prioritization\n",
        "\n",
        "### Integration with AI Agent\n",
        "\n",
        "All models are registered in the Snowflake Model Registry and can be:\n",
        "- Called via SQL UDFs (created in file 07)\n",
        "- Integrated with the Lereta Intelligence Agent\n",
        "- Used for batch scoring and real-time predictions\n",
        "- Monitored for performance drift\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Execute **file 07** (Python wrappers) to create SQL UDFs for each model\n",
        "2. Execute **file 08** (AI Agent creation) to integrate models with the agent\n",
        "3. Test model predictions through the AI Agent interface\n",
        "4. Monitor model performance and retrain as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify all models in registry\n",
        "print(\"\\n=== Registered Models ===\")\n",
        "registry.show_models()\n",
        "\n",
        "# Close session\n",
        "session.close()\n",
        "print(\"\\n✅ All models trained and registered successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
