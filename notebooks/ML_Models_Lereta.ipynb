{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lereta Intelligence Agent - ML Models\n",
        "\n",
        "**Training 3 Machine Learning Models for Tax & Flood Intelligence**\n",
        "\n",
        "This notebook trains 3 ML models for the Lereta Intelligence Agent:\n",
        "1. **TAX_DELINQUENCY_PREDICTOR** - Predicts property tax delinquency risk\n",
        "2. **CLIENT_CHURN_PREDICTOR** - Predicts client churn risk\n",
        "3. **LOAN_RISK_CLASSIFIER** - Classifies loans by risk level (LOW/MEDIUM/HIGH)\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "- Database: `LERETA_INTELLIGENCE`\n",
        "- Schema: `ANALYTICS`\n",
        "- Feature views created (V_TAX_DELINQUENCY_FEATURES, V_CLIENT_CHURN_FEATURES, V_LOAN_RISK_FEATURES)\n",
        "- Packages: `snowflake-ml-python`, `scikit-learn`, `xgboost`, `pandas`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.ml.modeling.ensemble import RandomForestClassifier\n",
        "from snowflake.ml.modeling.xgboost import XGBClassifier\n",
        "from snowflake.ml.modeling.preprocessing import OrdinalEncoder, StandardScaler\n",
        "from snowflake.ml.modeling.pipeline import Pipeline\n",
        "from snowflake.ml.registry import Registry\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✅ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get current session (Snowflake Notebook environment)\n",
        "session = Session.builder.getOrCreate()\n",
        "\n",
        "# Set context\n",
        "session.use_database(\"LERETA_INTELLIGENCE\")\n",
        "session.use_schema(\"ANALYTICS\")\n",
        "session.use_warehouse(\"LERETA_WH\")\n",
        "\n",
        "print(\"✅ Session configured\")\n",
        "print(f\"Database: {session.get_current_database()}\")\n",
        "print(f\"Schema: {session.get_current_schema()}\")\n",
        "print(f\"Warehouse: {session.get_current_warehouse()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Model Registry\n",
        "registry = Registry(\n",
        "    session=session,\n",
        "    database_name=\"LERETA_INTELLIGENCE\",\n",
        "    schema_name=\"ANALYTICS\"\n",
        ")\n",
        "\n",
        "print(\"✅ Model Registry initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 1: Tax Delinquency Prediction\n",
        "\n",
        "### Business Problem\n",
        "Predict which properties are likely to become delinquent on property taxes in the next 90 days. This enables proactive outreach to clients and borrowers to prevent delinquencies.\n",
        "\n",
        "### Features\n",
        "- Property characteristics (type, assessed value)\n",
        "- Tax amount and jurisdiction\n",
        "- Historical payment patterns\n",
        "- Days since last payment\n",
        "- Client service quality score\n",
        "- Loan characteristics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training dataset for tax delinquency prediction\n",
        "tax_delinquency_query = \"\"\"\n",
        "SELECT \n",
        "    t.tax_record_id,\n",
        "    -- Target variable\n",
        "    t.delinquent AS is_delinquent,\n",
        "    \n",
        "    -- Property features\n",
        "    p.property_type,\n",
        "    p.assessed_value,\n",
        "    p.flood_zone,\n",
        "    \n",
        "    -- Tax features\n",
        "    t.tax_amount,\n",
        "    tj.tax_rate,\n",
        "    tj.jurisdiction_type,\n",
        "    t.penalty_amount,\n",
        "    DATEDIFF('day', t.due_date, CURRENT_DATE()) AS days_since_due,\n",
        "    DATEDIFF('day', COALESCE(t.payment_date, CURRENT_DATE()), CURRENT_DATE()) AS days_since_last_payment,\n",
        "    \n",
        "    -- Loan features\n",
        "    l.loan_type,\n",
        "    l.loan_amount,\n",
        "    l.escrow_account,\n",
        "    l.loan_status,\n",
        "    \n",
        "    -- Client features\n",
        "    c.client_type,\n",
        "    c.service_quality_score,\n",
        "    c.client_status,\n",
        "    \n",
        "    -- Historical payment behavior\n",
        "    CASE WHEN t.payment_date IS NULL THEN 1 ELSE 0 END AS has_unpaid_taxes,\n",
        "    CASE WHEN t.payment_status = 'PAID' THEN 1 ELSE 0 END AS current_paid_status\n",
        "    \n",
        "FROM TAX_RECORDS t\n",
        "JOIN PROPERTIES p ON t.property_id = p.property_id\n",
        "JOIN LOANS l ON t.loan_id = l.loan_id\n",
        "JOIN CLIENTS c ON t.client_id = c.client_id\n",
        "JOIN TAX_JURISDICTIONS tj ON t.jurisdiction_id = tj.jurisdiction_id\n",
        "WHERE t.tax_year >= YEAR(CURRENT_DATE()) - 2\n",
        "    AND p.property_status = 'ACTIVE'\n",
        "    AND l.loan_status = 'ACTIVE'\n",
        "LIMIT 100000\n",
        "\"\"\"\n",
        "\n",
        "# Load data into Snowpark DataFrame\n",
        "tax_delinquency_df = session.sql(tax_delinquency_query)\n",
        "\n",
        "print(f\"Tax Delinquency Dataset Shape: {tax_delinquency_df.count()} rows\")\n",
        "tax_delinquency_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering for tax delinquency model\n",
        "categorical_features = ['property_type', 'flood_zone', 'jurisdiction_type', 'loan_type', 'client_type', 'loan_status', 'client_status']\n",
        "numerical_features = ['assessed_value', 'tax_amount', 'tax_rate', 'penalty_amount', 'days_since_due', \n",
        "                      'days_since_last_payment', 'loan_amount', 'service_quality_score', \n",
        "                      'has_unpaid_taxes', 'current_paid_status']\n",
        "\n",
        "# Split data into train/test (80/20)\n",
        "train_df, test_df = tax_delinquency_df.random_split([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"Training set: {train_df.count()} rows\")\n",
        "print(f\"Test set: {test_df.count()} rows\")\n",
        "\n",
        "# Build preprocessing pipeline\n",
        "tax_delinquency_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        ('encoder', OrdinalEncoder(input_cols=categorical_features, output_cols=categorical_features)),\n",
        "        ('scaler', StandardScaler(input_cols=numerical_features, output_cols=numerical_features)),\n",
        "        ('classifier', RandomForestClassifier(\n",
        "            input_cols=categorical_features + numerical_features,\n",
        "            label_cols=['IS_DELINQUENT'],\n",
        "            n_estimators=100,\n",
        "            max_depth=10,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Tax Delinquency Prediction Model...\")\n",
        "tax_delinquency_model = tax_delinquency_pipeline.fit(train_df)\n",
        "print(\"✅ Model trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate tax delinquency model\n",
        "predictions_df = tax_delinquency_model.predict(test_df)\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"\\n=== Tax Delinquency Model Performance ===\")\n",
        "accuracy = accuracy_score(df=predictions_df, y_true_col_name='IS_DELINQUENT', y_pred_col_name='OUTPUT_IS_DELINQUENT')\n",
        "precision = precision_score(df=predictions_df, y_true_col_name='IS_DELINQUENT', y_pred_col_name='OUTPUT_IS_DELINQUENT')\n",
        "recall = recall_score(df=predictions_df, y_true_col_name='IS_DELINQUENT', y_pred_col_name='OUTPUT_IS_DELINQUENT')\n",
        "f1 = f1_score(df=predictions_df, y_true_col_name='IS_DELINQUENT', y_pred_col_name='OUTPUT_IS_DELINQUENT')\n",
        "\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "# Show sample predictions\n",
        "predictions_df.select('TAX_RECORD_ID', 'IS_DELINQUENT', 'OUTPUT_IS_DELINQUENT').show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register tax delinquency model to Snowflake Model Registry\n",
        "session.use_schema(\"ANALYTICS\")\n",
        "\n",
        "# Create registry\n",
        "registry = Registry(session=session)\n",
        "\n",
        "# Register model\n",
        "model_name = \"TAX_DELINQUENCY_PREDICTOR\"\n",
        "model_version = \"v1\"\n",
        "\n",
        "print(f\"Registering model: {model_name}_{model_version}\")\n",
        "registry.log_model(\n",
        "    model=tax_delinquency_model,\n",
        "    model_name=model_name,\n",
        "    version_name=model_version,\n",
        "    comment=\"Predicts property tax delinquency risk using Random Forest. Features: property characteristics, tax amounts, payment history, client metrics.\"\n",
        ")\n",
        "\n",
        "print(f\"✅ Model {model_name} version {model_version} registered successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 2: Client Churn Prediction\n",
        "\n",
        "### Business Problem\n",
        "Identify clients (financial institutions) at risk of canceling their Lereta subscriptions. This enables proactive retention efforts and improved customer success.\n",
        "\n",
        "### Features\n",
        "- Subscription characteristics (tier, billing cycle, property count)\n",
        "- Service utilization patterns\n",
        "- Support ticket volume and satisfaction\n",
        "- Revenue and transaction trends\n",
        "- Client profile and status\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training dataset for client churn prediction\n",
        "session.use_schema(\"RAW\")\n",
        "\n",
        "client_churn_query = \"\"\"\n",
        "WITH client_metrics AS (\n",
        "    SELECT \n",
        "        c.client_id,\n",
        "        -- Target: Churn indicator (subscription expired and not renewed)\n",
        "        CASE \n",
        "            WHEN s.subscription_status IN ('EXPIRED', 'PENDING_RENEWAL') THEN 1 \n",
        "            ELSE 0 \n",
        "        END AS is_churned,\n",
        "        \n",
        "        -- Client features\n",
        "        c.client_type,\n",
        "        c.service_quality_score,\n",
        "        c.total_properties,\n",
        "        c.lifetime_value,\n",
        "        DATEDIFF('month', c.onboarding_date, CURRENT_DATE()) AS months_as_client,\n",
        "        \n",
        "        -- Subscription features\n",
        "        s.service_type,\n",
        "        s.subscription_tier,\n",
        "        s.billing_cycle,\n",
        "        s.monthly_price,\n",
        "        s.property_count_limit,\n",
        "        s.user_licenses,\n",
        "        s.advanced_analytics,\n",
        "        DATEDIFF('day', s.start_date, COALESCE(s.end_date, CURRENT_DATE())) AS subscription_duration_days,\n",
        "        \n",
        "        -- Support metrics (aggregated)\n",
        "        COUNT(DISTINCT st.ticket_id) AS total_support_tickets,\n",
        "        AVG(st.satisfaction_rating) AS avg_satisfaction_rating,\n",
        "        AVG(st.resolution_time_hours) AS avg_resolution_time,\n",
        "        SUM(CASE WHEN st.ticket_status = 'OPEN' THEN 1 ELSE 0 END) AS open_tickets,\n",
        "        \n",
        "        -- Revenue metrics\n",
        "        COUNT(DISTINCT t.transaction_id) AS total_transactions,\n",
        "        SUM(t.total_amount) AS total_revenue,\n",
        "        AVG(t.total_amount) AS avg_transaction_amount\n",
        "        \n",
        "    FROM CLIENTS c\n",
        "    LEFT JOIN SERVICE_SUBSCRIPTIONS s ON c.client_id = s.client_id\n",
        "    LEFT JOIN SUPPORT_TICKETS st ON c.client_id = st.client_id\n",
        "    LEFT JOIN TRANSACTIONS t ON c.client_id = t.client_id\n",
        "    WHERE c.client_status IN ('ACTIVE', 'SUSPENDED')\n",
        "        AND s.subscription_id IS NOT NULL\n",
        "    GROUP BY \n",
        "        c.client_id, c.client_type, c.service_quality_score, c.total_properties, \n",
        "        c.lifetime_value, c.onboarding_date, s.service_type, s.subscription_tier,\n",
        "        s.billing_cycle, s.monthly_price, s.property_count_limit, s.user_licenses,\n",
        "        s.advanced_analytics, s.subscription_status, s.start_date, s.end_date\n",
        ")\n",
        "SELECT * FROM client_metrics\n",
        "LIMIT 50000\n",
        "\"\"\"\n",
        "\n",
        "client_churn_df = session.sql(client_churn_query)\n",
        "\n",
        "print(f\"Client Churn Dataset Shape: {client_churn_df.count()} rows\")\n",
        "client_churn_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering for churn prediction\n",
        "categorical_features_churn = ['CLIENT_TYPE', 'SERVICE_TYPE', 'SUBSCRIPTION_TIER', 'BILLING_CYCLE']\n",
        "numerical_features_churn = [\n",
        "    'SERVICE_QUALITY_SCORE', 'TOTAL_PROPERTIES', 'LIFETIME_VALUE', 'MONTHS_AS_CLIENT',\n",
        "    'MONTHLY_PRICE', 'PROPERTY_COUNT_LIMIT', 'USER_LICENSES', 'SUBSCRIPTION_DURATION_DAYS',\n",
        "    'TOTAL_SUPPORT_TICKETS', 'AVG_SATISFACTION_RATING', 'AVG_RESOLUTION_TIME', 'OPEN_TICKETS',\n",
        "    'TOTAL_TRANSACTIONS', 'TOTAL_REVENUE', 'AVG_TRANSACTION_AMOUNT'\n",
        "]\n",
        "\n",
        "# Split data\n",
        "train_churn_df, test_churn_df = client_churn_df.random_split([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"Training set: {train_churn_df.count()} rows\")\n",
        "print(f\"Test set: {test_churn_df.count()} rows\")\n",
        "\n",
        "# Build churn prediction pipeline with XGBoost\n",
        "client_churn_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        ('encoder', OrdinalEncoder(input_cols=categorical_features_churn, output_cols=categorical_features_churn)),\n",
        "        ('scaler', StandardScaler(input_cols=numerical_features_churn, output_cols=numerical_features_churn)),\n",
        "        ('classifier', XGBClassifier(\n",
        "            input_cols=categorical_features_churn + numerical_features_churn,\n",
        "            label_cols=['IS_CHURNED'],\n",
        "            n_estimators=150,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Client Churn Prediction Model...\")\n",
        "client_churn_model = client_churn_pipeline.fit(train_churn_df)\n",
        "print(\"✅ Model trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate churn model\n",
        "churn_predictions_df = client_churn_model.predict(test_churn_df)\n",
        "\n",
        "print(\"\\n=== Client Churn Model Performance ===\")\n",
        "accuracy_churn = accuracy_score(df=churn_predictions_df, y_true_col_name='IS_CHURNED', y_pred_col_name='OUTPUT_IS_CHURNED')\n",
        "precision_churn = precision_score(df=churn_predictions_df, y_true_col_name='IS_CHURNED', y_pred_col_name='OUTPUT_IS_CHURNED')\n",
        "recall_churn = recall_score(df=churn_predictions_df, y_true_col_name='IS_CHURNED', y_pred_col_name='OUTPUT_IS_CHURNED')\n",
        "f1_churn = f1_score(df=churn_predictions_df, y_true_col_name='IS_CHURNED', y_pred_col_name='OUTPUT_IS_CHURNED')\n",
        "\n",
        "print(f\"Accuracy:  {accuracy_churn:.4f}\")\n",
        "print(f\"Precision: {precision_churn:.4f}\")\n",
        "print(f\"Recall:    {recall_churn:.4f}\")\n",
        "print(f\"F1 Score:  {f1_churn:.4f}\")\n",
        "\n",
        "churn_predictions_df.select('CLIENT_ID', 'IS_CHURNED', 'OUTPUT_IS_CHURNED').show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register churn model\n",
        "session.use_schema(\"ANALYTICS\")\n",
        "\n",
        "model_name_churn = \"CLIENT_CHURN_PREDICTOR\"\n",
        "model_version_churn = \"v1\"\n",
        "\n",
        "print(f\"Registering model: {model_name_churn}_{model_version_churn}\")\n",
        "registry.log_model(\n",
        "    model=client_churn_model,\n",
        "    model_name=model_name_churn,\n",
        "    version_name=model_version_churn,\n",
        "    comment=\"Predicts client churn risk using XGBoost. Features: subscription metrics, support satisfaction, revenue trends, service utilization.\"\n",
        ")\n",
        "\n",
        "print(f\"✅ Model {model_name_churn} version {model_version_churn} registered successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 3: Loan Risk Classification\n",
        "\n",
        "### Business Problem\n",
        "Classify loans into risk categories (Low, Medium, High) based on tax compliance, flood zone risks, and property characteristics. This helps prioritize monitoring efforts and identify high-risk portfolios.\n",
        "\n",
        "### Features\n",
        "- Flood zone and insurance requirements\n",
        "- Tax payment history and delinquency status\n",
        "- Loan-to-value ratio\n",
        "- Property type and assessed value\n",
        "- Escrow account status\n",
        "- Jurisdiction tax rates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training dataset for loan risk classification\n",
        "session.use_schema(\"RAW\")\n",
        "\n",
        "loan_risk_query = \"\"\"\n",
        "WITH loan_features AS (\n",
        "    SELECT \n",
        "        l.loan_id,\n",
        "        \n",
        "        -- Target variable: Risk Level (engineered from multiple factors)\n",
        "        CASE \n",
        "            WHEN (p.flood_zone IN ('AE', 'A', 'VE') AND fc.insurance_required = FALSE) \n",
        "                OR (t.delinquent = TRUE AND t.penalty_amount > 500) \n",
        "                OR (l.loan_status = 'FORECLOSED') THEN 'HIGH'\n",
        "            WHEN (p.flood_zone IN ('AE', 'A', 'VE') AND fc.insurance_required = TRUE) \n",
        "                OR (t.payment_status = 'PENDING' AND DATEDIFF('day', t.due_date, CURRENT_DATE()) > 30)\n",
        "                OR (l.escrow_account = FALSE AND t.delinquent = TRUE) THEN 'MEDIUM'\n",
        "            ELSE 'LOW'\n",
        "        END AS risk_level,\n",
        "        \n",
        "        -- Loan features\n",
        "        l.loan_type,\n",
        "        l.loan_amount,\n",
        "        l.loan_status,\n",
        "        l.escrow_account,\n",
        "        DATEDIFF('month', l.loan_date, CURRENT_DATE()) AS loan_age_months,\n",
        "        (l.loan_amount / NULLIF(p.assessed_value, 0))::DOUBLE AS loan_to_value_ratio,\n",
        "        \n",
        "        -- Property features\n",
        "        p.property_type,\n",
        "        p.assessed_value,\n",
        "        p.flood_zone,\n",
        "        p.property_state,\n",
        "        \n",
        "        -- Flood certification features\n",
        "        fc.insurance_required,\n",
        "        fc.life_of_loan_tracking,\n",
        "        CASE WHEN fz.risk_level = 'HIGH_RISK' THEN 1 ELSE 0 END AS high_flood_risk,\n",
        "        \n",
        "        -- Tax features\n",
        "        t.tax_amount,\n",
        "        t.delinquent,\n",
        "        t.penalty_amount,\n",
        "        tj.tax_rate,\n",
        "        tj.jurisdiction_type,\n",
        "        CASE WHEN t.payment_status = 'PAID' THEN 1 ELSE 0 END AS tax_paid_on_time,\n",
        "        DATEDIFF('day', t.due_date, COALESCE(t.payment_date, CURRENT_DATE())) AS days_payment_delay,\n",
        "        \n",
        "        -- Client features\n",
        "        c.client_type,\n",
        "        c.service_quality_score\n",
        "        \n",
        "    FROM LOANS l\n",
        "    JOIN PROPERTIES p ON l.property_id = p.property_id\n",
        "    LEFT JOIN FLOOD_CERTIFICATIONS fc ON l.loan_id = fc.loan_id\n",
        "    LEFT JOIN FLOOD_ZONES fz ON p.flood_zone = fz.zone_id\n",
        "    LEFT JOIN TAX_RECORDS t ON l.loan_id = t.loan_id\n",
        "    LEFT JOIN TAX_JURISDICTIONS tj ON t.jurisdiction_id = tj.jurisdiction_id\n",
        "    LEFT JOIN CLIENTS c ON l.client_id = c.client_id\n",
        "    WHERE l.loan_status IN ('ACTIVE', 'PAID_OFF', 'FORECLOSED')\n",
        "        AND p.property_status = 'ACTIVE'\n",
        ")\n",
        "SELECT * FROM loan_features\n",
        "WHERE risk_level IS NOT NULL\n",
        "LIMIT 100000\n",
        "\"\"\"\n",
        "\n",
        "loan_risk_df = session.sql(loan_risk_query)\n",
        "\n",
        "print(f\"Loan Risk Dataset Shape: {loan_risk_df.count()} rows\")\n",
        "loan_risk_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering for loan risk classification\n",
        "categorical_features_risk = [\n",
        "    'LOAN_TYPE', 'LOAN_STATUS', 'PROPERTY_TYPE', 'FLOOD_ZONE', \n",
        "    'PROPERTY_STATE', 'JURISDICTION_TYPE', 'CLIENT_TYPE'\n",
        "]\n",
        "numerical_features_risk = [\n",
        "    'LOAN_AMOUNT', 'LOAN_AGE_MONTHS', 'LOAN_TO_VALUE_RATIO', 'ASSESSED_VALUE',\n",
        "    'HIGH_FLOOD_RISK', 'TAX_AMOUNT', 'PENALTY_AMOUNT', 'TAX_RATE',\n",
        "    'TAX_PAID_ON_TIME', 'DAYS_PAYMENT_DELAY', 'SERVICE_QUALITY_SCORE'\n",
        "]\n",
        "\n",
        "# Split data\n",
        "train_risk_df, test_risk_df = loan_risk_df.random_split([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"Training set: {train_risk_df.count()} rows\")\n",
        "print(f\"Test set: {test_risk_df.count()} rows\")\n",
        "\n",
        "# Build loan risk classification pipeline\n",
        "loan_risk_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        ('encoder', OrdinalEncoder(input_cols=categorical_features_risk, output_cols=categorical_features_risk)),\n",
        "        ('scaler', StandardScaler(input_cols=numerical_features_risk, output_cols=numerical_features_risk)),\n",
        "        ('classifier', RandomForestClassifier(\n",
        "            input_cols=categorical_features_risk + numerical_features_risk,\n",
        "            label_cols=['RISK_LEVEL'],\n",
        "            n_estimators=120,\n",
        "            max_depth=12,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Loan Risk Classification Model...\")\n",
        "loan_risk_model = loan_risk_pipeline.fit(train_risk_df)\n",
        "print(\"✅ Model trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate loan risk model\n",
        "risk_predictions_df = loan_risk_model.predict(test_risk_df)\n",
        "\n",
        "print(\"\\n=== Loan Risk Classification Model Performance ===\")\n",
        "accuracy_risk = accuracy_score(df=risk_predictions_df, y_true_col_name='RISK_LEVEL', y_pred_col_name='OUTPUT_RISK_LEVEL')\n",
        "\n",
        "print(f\"Accuracy: {accuracy_risk:.4f}\")\n",
        "\n",
        "# Show sample predictions\n",
        "risk_predictions_df.select('LOAN_ID', 'RISK_LEVEL', 'OUTPUT_RISK_LEVEL', 'FLOOD_ZONE', 'DELINQUENT').show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register loan risk model\n",
        "session.use_schema(\"ANALYTICS\")\n",
        "\n",
        "model_name_risk = \"LOAN_RISK_CLASSIFIER\"\n",
        "model_version_risk = \"v1\"\n",
        "\n",
        "print(f\"Registering model: {model_name_risk}_{model_version_risk}\")\n",
        "registry.log_model(\n",
        "    model=loan_risk_model,\n",
        "    model_name=model_name_risk,\n",
        "    version_name=model_version_risk,\n",
        "    comment=\"Classifies loans into LOW/MEDIUM/HIGH risk categories using Random Forest. Features: flood zones, tax compliance, LTV ratio, property characteristics.\"\n",
        ")\n",
        "\n",
        "print(f\"✅ Model {model_name_risk} version {model_version_risk} registered successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "### Models Created\n",
        "\n",
        "1. **TAX_DELINQUENCY_PREDICTOR** (Random Forest)\n",
        "   - Predicts property tax delinquency risk\n",
        "   - Use case: Proactive client alerts, portfolio risk management\n",
        "\n",
        "2. **CLIENT_CHURN_PREDICTOR** (XGBoost)\n",
        "   - Predicts client subscription cancellation risk\n",
        "   - Use case: Customer retention, account management prioritization\n",
        "\n",
        "3. **LOAN_RISK_CLASSIFIER** (Random Forest)\n",
        "   - Classifies loans into risk levels (LOW/MEDIUM/HIGH)\n",
        "   - Use case: Portfolio risk assessment, monitoring prioritization\n",
        "\n",
        "### Integration with AI Agent\n",
        "\n",
        "All models are registered in the Snowflake Model Registry and can be:\n",
        "- Called via SQL UDFs (created in file 07)\n",
        "- Integrated with the Lereta Intelligence Agent\n",
        "- Used for batch scoring and real-time predictions\n",
        "- Monitored for performance drift\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Execute **file 07** (Python wrappers) to create SQL UDFs for each model\n",
        "2. Execute **file 08** (AI Agent creation) to integrate models with the agent\n",
        "3. Test model predictions through the AI Agent interface\n",
        "4. Monitor model performance and retrain as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify all models in registry\n",
        "print(\"\\n=== Registered Models ===\")\n",
        "registry.show_models()\n",
        "\n",
        "# Close session\n",
        "session.close()\n",
        "print(\"\\n✅ All models trained and registered successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
